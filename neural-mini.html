
<!-- saved from url=(0036)https://mathsites.unibe.ch/siamag19/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title> SIAM AG 19: Algebraic Neural Coding Minisymposium</title>
<link rel="stylesheet" type="text/css" href="./SIAM-webpage_files/css">
<link rel="stylesheet" type="text/css" href="./SIAM-webpage_files/main.css">
<script type="text/javascript" async="" src="./SIAM-webpage_files/analytics.js"></script><script language="JavaScript">
<!-- Begin
	if (window != top) top.location.href = location.href;
// End --> 
</script></head>



<body>

<!--Here comes the header picture-->
<img src="./SIAM-webpage_files/header2.png" width="91%">


<!--The variable $location will be used to switch to subpages-->

<table border="0" width="87%">

<!--Now the navigation part-->

<tbody><tr><td valign="top" width="24%">

<a href="https://mathsites.unibe.ch/siamag19/index.php?location=home"><u>Conference Home</u></a><br>

<br>
<br>
<br>

<img src="./SIAM-webpage_files/sponsors2.png" width="76%">

</td>

<!--And then the content-->


<td valign="top" width="61%">

<p>
  <h2>Algebraic Neural Coding Mini-Symposium</h2>
  Welcome to the Algebraic Neural Coding Mini-Symposium at SIAM AG 2019!<br>
  We are excited to bring together a group of mathematicians doing
  innovative work in this dynamic field. We hope this setting will enable experts in
  commutative algebra, geometry, topology, and combinatorics to
  connect and collaborate on problems related to neural codes, neural rings, and neural networks.
</p>
<p>
  <i>Organizers:</i> Nora Youngs & Zvi Rosen <br>
  <i>Location:</i> Unitobler, F-105<br>
  <i>Dates & Times:</i> Tuesday, July 9 and Wednesday July 10, 10 am - 12 pm.
</p>

<h2>Schedule</h2>
<p>
  <table style="width:100%">
    <tr>
      <th>   </th>
      <th> Tuesday (July 9) </th>
      <th> Wednesday (July 10) </th>      
    </tr>
    <tr>
      <td> 10:00-10:25 </td>
      <td> <a href="#curto">Carina Curto</a> </td>
      <td> <a href="#jeffs">R. Amzi Jeffs</a> </td>
    </tr>
    <tr>
    <tr>
      <td> 10:30-10:55 </td>
      <td>  <a href="#morrison">Katherine Morrison</a> </td>
      <td>  <a href="#lienkaemper">Caitlin Lienkaemper</a> </td>
    </tr>
    <tr>
      <td> 11:00-11:25 </td>
      <td>  <a href="#itskov">Vladimir Itskov</a> </td>
      <td>  <a href="#obatake">Nida Obatake</a> </td>
    </tr>
    <tr>
    <tr>
      <td>11:30-11:55 </td>
      <td>  <a href="#kunin">Alexander Kunin</a></td>
      <td>  <a href="#davis">Robert Davis</a></td>
    </tr>	
  </table>

</p>

<br>
<br>
<p>
<h2>Titles & Abstracts</h2>

<a name="curto"><h3>Flexible Motifs in Threshold-Linear Networks</h3></a>
<b>Carina Curto</b>,
The Pennsylvania State University
<br> <br>

Threshold-linear networks (TLNs) are popular models of recurrent networks used to model neural activity in the brain. The state space in these networks is naturally partitioned into regions defined by an associated hyperplane arrangement. The combinatorial properties of this arrangement, as captured by an oriented matroid, provide strong constraints on the network's dynamics. In recent work, we have studied how the graph of a TLN constrains the possible fixed points of the network by providing constraints on the combinatorics of the hyperplane arrangement. Here we study the case of flexible motifs, where the graph allows multiple possibilities for the set of fixed points FP(W), depending on the choice of connectivity matrix W. In particular, we find that mutations of oriented matroids correspond naturally to bifurcations in the dynamics. Flexible motifs are interesting from a neuroscience perspective because they allow us to study the effects of sensory and state-dependent modulation on the dynamics of neural ensembles.

 
<a name="morrison"><h3>Robust Motifs in Threshold-Linear Networks</h3></a>
<b>Katherine Morrison</b>,
University of Northern Colorado
<br> <br>

Networks of neurons in the brain often exhibit complex patterns of activity that are shaped by the intrinsic structure of the network. How does the precise connectivity structure of the network influence these patterns of activity? We address this question in the context of threshold-linear networks, a commonly used model of recurrent neural networks. We identify constraints on the dynamics that arise from network architecture and are independent of the specific values of connection strengths. By appealing to an associated hyperplane arrangement, we find families of robust motifs, which are graphs where the collection of fixed points of the corresponding networks is fully determined by the graph structure, irrespective of the particular connection strengths. These motifs provide a direct link between network structure and function, and provide new insights into how connectivity may shape dynamics in real neural circuits.

 
<a name="itskov"><h3>An Algebraic Perceptron and the Neural Ideals</h3></a>

<b>Vladimir Itskov</b>,
The Pennsylvania State University
<br> <br>

Feedforward neural networks have been widely used in machine-learning and theoretical neuroscience. The paradigm of "deep learning", that makes use of many consecutive layers of feedforward networks, has achieved impressive engineering success in the past two decades. However, a theoretical understanding of many-layer feedforward networks is still mostly lacking. While each layer of a feedforward network can be understood via the geometry of an hyperplane arrangement, satisfactory understanding the mathematical properties of many-layered networks remains elusive. 

We propose a generalization of the perceptron, i.e. a single layer of a feedforward network. This perceptron is best described via a neural ideal, i.e. an ideal in the ring of functions on the Boolean lattice. It turns out that many machine-learning problems can be converted into purely algebraic problems about neural ideals. This opens up a new avenue of developing a commutative algebra-based toolbox for machine-learning. In my talk I will explain the connection between these two subjects and also give a concrete example of translating a machine-learning problem into commutative algebra.

 
<a name="kunin"><h3>Properties of Hyperplane Neural Codes</h3></a>

<b>Alexander Kunin</b>,
The Pennsylvania State University
<br> <br>

The firing patterns of neurons in sensory systems give rise to combinatorial codes, i.e. subsets of the boolean lattice. These firing patterns represent the abstract intersection patterns of subsets of a Euclidean space, and an open problem is identifying the combinatorial properties of neural codes which distinguish the geometric properties of the corresponding subsets. We introduce the polar complex, a simplicial complex associated to any combinatorial code, and relate its associated Stanley-Reisner ring to the ring of <img src="https://tex.s2cms.ru/svg/%5Cmathbb%7BF%7D_2" alt="\mathbb{F}_2" />-valued functions on the code to identify some distinguishing characteristics of codes arising from feed-forward neural networks. In particular, we show the associated ring is Cohen-Macaulay, and make connections to other questions in the study of boolean functions.

<a name="jeffs"><h3>Sunflowers of Convex Sets and New Obstructions to Convexity</h3></a>

<b>R. Amzi Jeffs</b>,
University of Washington
<br> <br>

Any collection of convex open sets in <img src="https://tex.s2cms.ru/svg/%5Cmathbb%7BR%7D%5Ed" alt="\mathbb{R}^d" /> gives rise to an associated neural code. The question of which codes can be realized in this way has been an open problem for a number of years, and although recent literature has described rich combinatorial and geometric obstructions to convexity, a full classification (even conjectural) is far out of reach. I will describe some new obstructions based on sunflowers of convex open sets, and show how these obstructions differ fundamentally from those which have been investigated previously.
 
<a name="lienkaemper"><h3>Convex Codes and Oriented Matroids</h3></a>

<b>Caitlin Lienkaemper</b>,
The Pennsylvania State University
<br> <br>

Convex neural codes describe the intersection patterns of collections of convex open sets. Representable oriented matroids describe the intersection patterns of collections of half spaces—that is, of convex sets with convex complements. It is thus natural to view convex codes as a generalization of oriented matroids. In this talk, we will make this relationship precise. First, using a new notion of neural code morphism, we show that a code has a realization with convex polytopes if and only if it is the image of a representable matroid under such a morphism. This allows us to translate the problem of whether a code has a convex polytope realization into a matroid completion problem. Next, we enumerate all neural codes which are images of small representable matroids, and use the relationship between convex codes and oriented matroids to define new signatures of convexity and non-convexity. This is joint work with Alex Kunin and Zvi Rosen.

 
<a name="obatake"><h3>Sufficient Conditions for 1- and 2- Inductively Pierced Codes</h3></a>

<b>Nida Obatake</b>,
Texas A&M University
<br> <br>

Neural codes are binary codes in <img src="https://tex.s2cms.ru/svg/%5C%7B0%2C1%5C%7D%5En" alt="\{0,1\}^n" />; here we focus on the ones which represent the firing patterns of a type of neurons called place cells. There is much interest in determining which neural codes can be realized by a collection of convex sets. However, drawing representations of these convex sets, particularly as the number of neurons in a code increases, can be very difficult. Nevertheless, for a class of codes that are said to be k-inductively pierced for k = 0, 1, 2 there is an algorithm for drawing Euler diagrams. Here we use the toric ideal of a code to show sufficient conditions for a code to be 1- or 2-inductively pierced, so that we may use the existing algorithm to draw realizations of such codes.

 
<a name="davis"><h3>Progress Toward a Classification of Inductively Pierced Codes via Polyhedra</h3></a>

<b>Robert Davis</b>,
Harvey Mudd College
<br> <br>

A difficult problem in the field of combinatorial neural codes is to determine when a given code can be represented in the plane as intersections of convex sets and their complements. If the code is 2-inductively pierced, then there exists a polynomial-time algorithm which constructs such a representation in the plane and which uses closed discs as the convex sets. Recently, Gross, Obatake, and Youngs provided a way to classify 2-inductively pierced codes for up to three neurons by considering a special weight order on ideals of polynomials associated to the codes. In this talk, we present progress toward extending their result for an arbitrary number of neurons. We focus on the use of state polytopes of homogeneous toric ideals, which encode their distinct reduced Gröbner bases. It is the properties of these bases that we aim to connect to being 2-inductively pierced.



</p>

</td>

</tr></tbody></table>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async="" src="./SIAM-webpage_files/js"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-2397282-4');
</script>






</body></html>
